# Apache Spark RDD 知识点总结

## 1. RDD 定义与核心概念
**RDD (Resilient Distributed Datasets)** 是 Spark 的核心数据抽象，相当于一种分布式的、可靠的虚拟内存。
* **不可变 (Immutable)**：对象集合一旦创建，内容不可更改。
* **位置灵活**：可以存储在集群的内存中（快）或磁盘上（持久化）。
* **并行操作**：通过 `map`、`filter` 等并行转换算子进行构建和处理。

## 2. 容错机制：Lineage (血统)
RDD 采用了一种独特的容错策略，不同于 HDFS 的数据复制（Replication）：
* **记录操作历史**：RDD 记录了它是如何从上一步数据计算而来的（Lineage）。
* **自动重建**：当节点故障导致数据丢失时，Spark 不会去读备份，而是根据 Lineage 重新执行计算步骤，恢复丢失的数据分区。
* **优势**：节省了存储多份数据副本的空间和网络传输开销。

## 3. 工作流与操作 (以日志挖掘为例)
Spark 的操作分为“转换”和“动作”：
1.  **Base RDD**: 从 HDFS 读取文件 (`spark.textFile`)。
2.  **Transformation (转换)**: 
    * `filter`: 筛选出包含 "ERROR" 的日志。
    * `map`: 解析日志内容，提取关键字段。
    * *特点*：这些操作是惰性的（Lazy），只记录计划，不立即执行。
3.  **Caching (缓存)**: 使用 `.cache()` 将中间结果驻留在内存中，避免重复计算。
4.  **Action (动作)**: 如 `.count()`，触发真正的计算任务，将任务分发给 Worker 执行并返回结果。

## 4. 性能对比：Spark vs. Hadoop
Spark 的最大优势在于处理**迭代式算法**（如机器学习中的逻辑回归）：
* **Hadoop MapReduce**: 每一次迭代都需要从磁盘读取数据，计算完再写入磁盘。
    * *耗时*: 约 110秒/次迭代。
* **Spark**: 数据加载一次后常驻内存。
    * *耗时*: 首次迭代 80秒（含加载），后续迭代仅需 **1秒**。
    * *结果*: 对于需要反复利用数据的任务，Spark 比 Hadoop 快 100 倍以上。